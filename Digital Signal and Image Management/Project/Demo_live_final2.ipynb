{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\done-\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import itertools \n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from subprocess import check_output\n",
    "import cv2\n",
    "import re\n",
    "from PIL import Image\n",
    "from scipy import ndimage\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn import metrics\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils, to_categorical\n",
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras import optimizers\n",
    "from keras import layers\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "import IPython.display as ipd # Solo per Jupyter Notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import wavfile as wav\n",
    "from scipy.fftpack import rfft, fft, irfft, ifft, fftfreq\n",
    "from scipy.signal import fftconvolve\n",
    "import  tarfile\n",
    "import urllib.request as request\n",
    "import os, sys\n",
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import librosa\n",
    "import sounddevice as sd\n",
    "\n",
    "from keras.layers import LSTM, Input, Embedding, Conv1D, SpatialDropout1D\n",
    "from keras.layers import MaxPooling1D, Dense, GRU, Flatten, Dropout, BatchNormalization, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam, Adadelta\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, ReduceLROnPlateau\n",
    "import keras.backend as K\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from utils import plot_confusion_matrix, whiten, preprocess_instances\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "rate = 4000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading image model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = 224\n",
    "channels=3\n",
    "batch_size = 128\n",
    "num_classes = 2\n",
    "data_augmentation = True\n",
    "target_shape = (1,224,224,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3), pooling = 'max')\n",
    "for i in range(0, len(pretrained_model.layers)):\n",
    "    if i < 15:\n",
    "        pretrained_model.layers[i].trainable = False\n",
    "\n",
    "firstnet = Sequential()\n",
    "\n",
    "firstnet.add(pretrained_model)\n",
    "firstnet.add(Dense(1024, activation= 'relu'))\n",
    "firstnet.add(Dense(512, activation= 'relu'))\n",
    "firstnet.add(Dropout(0.1))\n",
    "firstnet.add(Dense(256, activation= 'relu'))\n",
    "firstnet.add(Dense(128, activation= 'relu'))\n",
    "firstnet.add(Dropout(0.1))\n",
    "firstnet.add(Dense(64, activation= 'relu'))\n",
    "firstnet.add(Dense(32, activation= 'relu'))\n",
    "firstnet.add(Dropout(0.1))\n",
    "firstnet.add(Dense(16, activation= 'relu'))\n",
    "firstnet.add(Dense(8, activation= 'relu'))\n",
    "firstnet.add(Dense(4, activation = 'relu'))\n",
    "firstnet.add(Dense(2, activation= 'sigmoid'))\n",
    "\n",
    "firstnet.load_weights('sex_weightsDUE2.h5')\n",
    "firstnet.compile(loss='binary_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=0.0001, decay= 0.0000001),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model2 = VGG16(weights='imagenet', include_top=False, input_shape=(224,224,3), pooling = 'max')\n",
    "for i in range(0, len(pretrained_model2.layers)):\n",
    "    if i < 15:\n",
    "        pretrained_model2.layers[i].trainable = False\n",
    "\n",
    "secondnet = Sequential()\n",
    "\n",
    "secondnet.add(pretrained_model2)\n",
    "secondnet.add(Dense(2048, activation= 'relu'))\n",
    "secondnet.add(Dense(1024, activation= 'relu'))\n",
    "secondnet.add(Dropout(0.1))\n",
    "secondnet.add(Dense(512, activation= 'relu'))\n",
    "secondnet.add(Dense(256, activation= 'relu'))\n",
    "secondnet.add(Dropout(0.1))\n",
    "secondnet.add(Dense(128, activation= 'relu'))\n",
    "secondnet.add(Dense(64, activation= 'relu'))\n",
    "secondnet.add(Dropout(0.1))\n",
    "secondnet.add(Dense(32, activation= 'relu'))\n",
    "secondnet.add(Dense(16, activation= 'relu'))\n",
    "secondnet.add(Dense(4, activation = 'softmax'))\n",
    "\n",
    "secondnet.load_weights('4th_tranche_best_model4classi.h5')\n",
    "secondnet.compile(loss='categorical_crossentropy',\n",
    "          optimizer=optimizers.SGD(lr=0.0001, momentum= 0.9, decay=0.000001),\n",
    "          metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender = ['Male', 'Female']\n",
    "ethnicity = ['White', 'Black', 'Asian', 'Indian']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading audio model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "langs = [\n",
    "        'German', \n",
    "        'English',\n",
    "        'Spanish',\n",
    "        'French', \n",
    "        'Italian',\n",
    "        'Russian'\n",
    "    ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_random_step(voice, dt):\n",
    "    start = np.random.randint(0, int(len(voice)-dt*rate))\n",
    "    end = int(start + dt*rate)\n",
    "    return voice[start:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepro_audio_setting(langs, dt):\n",
    "    def prepro_audio_streaming(live_audio):   \n",
    "        live_steps = np.array([pick_random_step(live_audio.astype(np.float), dt) for i in range(10)])\n",
    "        prepro = preprocess_instances(1, whitening=True)\n",
    "        X_norm = prepro(live_steps.reshape(-1, 16000, 1))\n",
    "        return X_norm\n",
    "    return prepro_audio_streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = 'best_model_saved.hdf5'\n",
    "best_model = load_model(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_audio = prepro_audio_setting(langs, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function for models inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(im, audio):\n",
    "    img = im\n",
    "    img =  np.asarray(img)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = img/255.0\n",
    "    try:\n",
    "        y = firstnet.predict(img.reshape(target_shape))\n",
    "        predict = np.argmax(y, axis=1)\n",
    "        gen = gender[int(predict)]\n",
    "        y2 = secondnet.predict(img.reshape(target_shape))\n",
    "        predict2 = np.argmax(y2, axis=1)\n",
    "        ethn = ethnicity[int(predict2)]\n",
    "    except:\n",
    "        ethn, gen = 'None', 'None'\n",
    "    my_input = prepro_audio(audio[::2])\n",
    "    my_preds = best_model.predict(my_input.reshape(-1,16000,1))\n",
    "    pred = np.mean(my_preds, axis=0)\n",
    "    lang_id = np.argmax(pred)\n",
    "    my_lang = langs[lang_id]\n",
    "    if np.median(np.abs(audio))< 0.005 or np.median(np.abs(audio)[-24000:])< 0.002:\n",
    "        output_lang = 'Say something!'\n",
    "    else:\n",
    "        output_lang = my_lang\n",
    "    return ethn, gen, output_lang\n",
    "\n",
    "def write_frame(im, ethn, gen, lang):\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(im,gen,(0,50), font, 1,(0,0,0),2,cv2.LINE_AA)\n",
    "    cv2.putText(im,ethn,(0,100), font, 1,(0,0,0),2,cv2.LINE_AA)\n",
    "    cv2.putText(im,lang,(0,150), font, 1,(0,0,0),2,cv2.LINE_AA)\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def live_testing():\n",
    "    prepro_audio = prepro_audio_setting(langs, 4)\n",
    "    myrecording = sd.rec(int(8.1*8000),samplerate=8000, channels=1)[:, 0]\n",
    "    t_rec = time.time() \n",
    "    \n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    k=0\n",
    "    while time.time() - t_rec < 5.1:\n",
    "        k += 1\n",
    "    ethnicity, gender = 'None', 'None'\n",
    "    while True:\n",
    "        r, frame = cap.read()\n",
    "        if time.time() - t_rec > 5.1: \n",
    "            t_rec = time.time() \n",
    "            ethnicity, gender, lang = process_frame(frame, myrecording)\n",
    "            myrecording = sd.rec(int(5.*8000),samplerate=8000, channels=1)[:, 0]\n",
    "        \n",
    "        frame = write_frame(frame, ethnicity, gender, lang)   \n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        \n",
    "        if cv2.waitKey(20) & 0xFF == ord('q'):break\n",
    "            \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\done-\\Anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:3250: RuntimeWarning: Invalid value encountered in median\n",
      "  r = func(a, **kwargs)\n",
      "C:\\Users\\done-\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: RuntimeWarning: invalid value encountered in less\n"
     ]
    }
   ],
   "source": [
    "live_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
